# fast-llama
Repo for learning fast inferencing techniques for llama2

# Setup

```bash
conda create -n fast-llama python=3.8 -y && \
pip install vllm
```